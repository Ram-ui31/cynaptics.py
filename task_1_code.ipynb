{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "84fe659d-deec-40cd-b687-726600e55ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hc/f67znl516xd8dpdwl3_517440000gn/T/ipykernel_94452/4209110604.py:5: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchaudio.transforms import MFCC\n",
    "import torchaudio.transforms as T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "129e03ca-1f2c-4019-9f32-e8e5874df311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import random\n",
    "\n",
    "class AugmentWrapper(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.freq_mask = T.FrequencyMasking(freq_mask_param=8)\n",
    "        self.time_mask = T.TimeMasking(time_mask_param=8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() > 0.5:\n",
    "            x = self.freq_mask(x)\n",
    "        if random.random() > 0.5:\n",
    "            x = self.time_mask(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "eec14020-df35-43ff-a35c-5e4245bdd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "EPOCHS=75\n",
    "LEARNING_RATE=0.0001\n",
    "SAMPLE_RATE=22050\n",
    "NUM_SAMPLES=22050*4\n",
    "N_MFCC=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b9b0e93e-7fa3-49a9-b286-6bc32a603423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        \n",
    "        self.audio_files = []\n",
    "        self.class_mapping = sorted(\n",
    "    [d for d in os.listdir(audio_dir) if not d.startswith(\".\")]\n",
    ")\n",
    "\n",
    "        for label_name in self.class_mapping:\n",
    "            class_folder = os.path.join(audio_dir, label_name)\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.audio_files.append((os.path.join(class_folder, file), label_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_path, label_name = self.audio_files[index]\n",
    "        label = self.class_mapping.index(label_name)\n",
    "\n",
    "    # --- Load ---\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        signal = signal.to(self.device)\n",
    "\n",
    "    # --- Preprocess ---\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "\n",
    "    # --- Apply MFCC or MelSpectrogram ---\n",
    "        signal = self.transformation(signal)\n",
    "\n",
    "    # --- Optional augmentation (only during training) ---\n",
    "        if getattr(self, \"training\", False):   # avoids error if attribute not set\n",
    "            signal = self.augment(signal)\n",
    "\n",
    "        return signal, label\n",
    "\n",
    "\n",
    "    # helper functions\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "524e3b9f-444f-4e77-907f-521b2a207ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        \n",
    "        self.audio_files = []\n",
    "        self.class_mapping = sorted(\n",
    "    [d for d in os.listdir(audio_dir) if not d.startswith(\".\")]\n",
    ")\n",
    "\n",
    "        for label_name in self.class_mapping:\n",
    "            class_folder = os.path.join(audio_dir, label_name)\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.audio_files.append((os.path.join(class_folder, file), label_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        audio_path, label_name = self.audio_files[index]\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        label = self.class_mapping.index(label_name)\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bc447147-a787-4116-aafc-7fa360e76a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        \n",
    "        self.audio_files = []\n",
    "        self.class_mapping = sorted(\n",
    "    [d for d in os.listdir(audio_dir) if not d.startswith(\".\")]\n",
    ")\n",
    "\n",
    "        for label_name in self.class_mapping:\n",
    "            class_folder = os.path.join(audio_dir, label_name)\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.audio_files.append((os.path.join(class_folder, file), label_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        audio_path, label_name = self.audio_files[index]\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        label = self.class_mapping.index(label_name)\n",
    "        \n",
    "        return signal, label\n",
    "\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6010ab2a-ed9f-4687-919c-7ae373b3863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_data,batch_size):\n",
    "    return DataLoader(train_data,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "60a06450-09a4-4ade-92a1-289509c8c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, data_loader, optimiser, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d2528ee8-3782-4a54-b763-0401617bad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    \"\"\"Calculates loss and accuracy on a given dataloader.\"\"\"\n",
    "    model.eval() \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for input, target in dataloader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            prediction = model(input)\n",
    "            \n",
    "            \n",
    "            total_loss += loss_fn(prediction, target).item()\n",
    "            \n",
    "           \n",
    "            _, predicted_class = torch.max(prediction, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted_class == target).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    model.train()\n",
    "    return avg_loss, accuracy\n",
    "def train(model, train_dataloader, validation_dataloader, loss_fn, optimizer, device, epochs, patience=10):\n",
    "    best_validation_loss = float('inf')\n",
    "    patience_counter = 0  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        current_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        \n",
    "        for input, target in train_dataloader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(input)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            current_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = current_train_loss / len(train_dataloader)\n",
    "\n",
    "        \n",
    "        avg_val_loss, val_accuracy = evaluate(model, validation_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "       \n",
    "        if avg_val_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_val_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(model.state_dict(), \"fest_model_best.pth\")\n",
    "            print(\"  Saved Model - New Best Validation Loss!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"   No improvement ({patience_counter}/{patience})\")\n",
    "\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n⏹️Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFINISHED TRAINING!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "186609dc-892b-45f0-93a2-3a38bf7d2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    def __init__(self, input_shape=(1, 40, 44)): \n",
    "        super().__init__()\n",
    "        self.Conv1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.Conv4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.25) \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.rand(1, *input_shape)\n",
    "            x = self.Conv1(dummy_input)\n",
    "            x = self.Conv2(x)\n",
    "            x = self.Conv3(x)\n",
    "            x = self.Conv4(x)\n",
    "            flattened_size = self.flatten(x).shape[1]\n",
    "            \n",
    "        print(f\"INFO: Dynamically calculated flattened size is: {flattened_size}\")\n",
    "        self.linear = nn.Linear(6144, 5) \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.Conv1(input_data)\n",
    "        x = self.Conv2(x)\n",
    "        x = self.Conv3(x)\n",
    "        x = self.Conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b2a955b6-edaa-4428-8819-ff7bd75b2711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "INFO: Dynamically calculated flattened size is: 2048\n",
      "CNNNetwork(\n",
      "  (Conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Conv4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (linear): Linear(in_features=6144, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print(\"Using CPU\")\n",
    "    device=\"cpu\"\n",
    "    mfcc_transformation = MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=N_MFCC,  \n",
    "    melkwargs={\n",
    "        \"n_fft\": 1024,\n",
    "        \"hop_length\": 512,\n",
    "        \"n_mels\": 64, \n",
    "    }\n",
    "    )\n",
    "    dataset = AudioDataset(\n",
    "    audio_dir=\"/Users/ramupadhyay/Desktop/train\",\n",
    "    transformation=mfcc_transformation,\n",
    "    target_sample_rate=SAMPLE_RATE,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    device=\"cpu\"\n",
    "    )\n",
    "    train_dataloader=create_data_loader(dataset,BATCH_SIZE)\n",
    "\n",
    "    TRAIN_RATIO = 0.85 \n",
    "    VALIDATION_RATIO = 0.15\n",
    "\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(TRAIN_RATIO * dataset_size)\n",
    "    validation_size = dataset_size - train_size\n",
    "    train_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [train_size, validation_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    cnn=CNNNetwork().to(device)\n",
    "    print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8a5c3290-1104-4585-93f3-be74e95c02c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramupadhyay/audioenv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/75\n",
      "  Training Loss: 1.3360\n",
      "  Validation Loss: 1.2142, Validation Accuracy: 60.23%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 2/75\n",
      "  Training Loss: 0.9971\n",
      "  Validation Loss: 0.9216, Validation Accuracy: 66.60%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 3/75\n",
      "  Training Loss: 0.8022\n",
      "  Validation Loss: 0.7295, Validation Accuracy: 74.13%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 4/75\n",
      "  Training Loss: 0.7050\n",
      "  Validation Loss: 0.6348, Validation Accuracy: 77.22%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 5/75\n",
      "  Training Loss: 0.6272\n",
      "  Validation Loss: 0.5741, Validation Accuracy: 78.76%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 6/75\n",
      "  Training Loss: 0.5468\n",
      "  Validation Loss: 0.5090, Validation Accuracy: 81.85%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 7/75\n",
      "  Training Loss: 0.4971\n",
      "  Validation Loss: 0.4850, Validation Accuracy: 82.43%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 8/75\n",
      "  Training Loss: 0.4745\n",
      "  Validation Loss: 0.4979, Validation Accuracy: 81.27%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 9/75\n",
      "  Training Loss: 0.4264\n",
      "  Validation Loss: 0.4314, Validation Accuracy: 85.52%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 10/75\n",
      "  Training Loss: 0.3917\n",
      "  Validation Loss: 0.4274, Validation Accuracy: 85.14%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 11/75\n",
      "  Training Loss: 0.3742\n",
      "  Validation Loss: 0.3925, Validation Accuracy: 86.10%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 12/75\n",
      "  Training Loss: 0.3655\n",
      "  Validation Loss: 0.4302, Validation Accuracy: 84.17%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 13/75\n",
      "  Training Loss: 0.3148\n",
      "  Validation Loss: 0.3577, Validation Accuracy: 88.03%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 14/75\n",
      "  Training Loss: 0.2930\n",
      "  Validation Loss: 0.3859, Validation Accuracy: 86.10%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 15/75\n",
      "  Training Loss: 0.2828\n",
      "  Validation Loss: 0.3485, Validation Accuracy: 88.42%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 16/75\n",
      "  Training Loss: 0.2555\n",
      "  Validation Loss: 0.3333, Validation Accuracy: 90.15%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 17/75\n",
      "  Training Loss: 0.2490\n",
      "  Validation Loss: 0.3307, Validation Accuracy: 88.61%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 18/75\n",
      "  Training Loss: 0.2280\n",
      "  Validation Loss: 0.3448, Validation Accuracy: 89.77%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 19/75\n",
      "  Training Loss: 0.2202\n",
      "  Validation Loss: 0.2903, Validation Accuracy: 90.73%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 20/75\n",
      "  Training Loss: 0.2002\n",
      "  Validation Loss: 0.3063, Validation Accuracy: 90.54%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 21/75\n",
      "  Training Loss: 0.1959\n",
      "  Validation Loss: 0.2748, Validation Accuracy: 90.73%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 22/75\n",
      "  Training Loss: 0.1825\n",
      "  Validation Loss: 0.2615, Validation Accuracy: 91.70%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 23/75\n",
      "  Training Loss: 0.1644\n",
      "  Validation Loss: 0.2717, Validation Accuracy: 91.89%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 24/75\n",
      "  Training Loss: 0.1523\n",
      "  Validation Loss: 0.2920, Validation Accuracy: 90.54%\n",
      "   No improvement (2/10)\n",
      "\n",
      "Epoch 25/75\n",
      "  Training Loss: 0.1459\n",
      "  Validation Loss: 0.2848, Validation Accuracy: 91.51%\n",
      "   No improvement (3/10)\n",
      "\n",
      "Epoch 26/75\n",
      "  Training Loss: 0.1372\n",
      "  Validation Loss: 0.3199, Validation Accuracy: 89.77%\n",
      "   No improvement (4/10)\n",
      "\n",
      "Epoch 27/75\n",
      "  Training Loss: 0.1279\n",
      "  Validation Loss: 0.2653, Validation Accuracy: 91.12%\n",
      "   No improvement (5/10)\n",
      "\n",
      "Epoch 28/75\n",
      "  Training Loss: 0.1194\n",
      "  Validation Loss: 0.2297, Validation Accuracy: 91.70%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 29/75\n",
      "  Training Loss: 0.1186\n",
      "  Validation Loss: 0.2365, Validation Accuracy: 91.89%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 30/75\n",
      "  Training Loss: 0.1114\n",
      "  Validation Loss: 0.2536, Validation Accuracy: 92.86%\n",
      "   No improvement (2/10)\n",
      "\n",
      "Epoch 31/75\n",
      "  Training Loss: 0.1058\n",
      "  Validation Loss: 0.2399, Validation Accuracy: 93.24%\n",
      "   No improvement (3/10)\n",
      "\n",
      "Epoch 32/75\n",
      "  Training Loss: 0.0961\n",
      "  Validation Loss: 0.2139, Validation Accuracy: 93.05%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 33/75\n",
      "  Training Loss: 0.0883\n",
      "  Validation Loss: 0.2070, Validation Accuracy: 93.05%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 34/75\n",
      "  Training Loss: 0.0892\n",
      "  Validation Loss: 0.2232, Validation Accuracy: 93.63%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 35/75\n",
      "  Training Loss: 0.0834\n",
      "  Validation Loss: 0.2133, Validation Accuracy: 93.82%\n",
      "   No improvement (2/10)\n",
      "\n",
      "Epoch 36/75\n",
      "  Training Loss: 0.0794\n",
      "  Validation Loss: 0.2290, Validation Accuracy: 93.63%\n",
      "   No improvement (3/10)\n",
      "\n",
      "Epoch 37/75\n",
      "  Training Loss: 0.0755\n",
      "  Validation Loss: 0.2057, Validation Accuracy: 92.86%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 38/75\n",
      "  Training Loss: 0.0713\n",
      "  Validation Loss: 0.2020, Validation Accuracy: 93.63%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 39/75\n",
      "  Training Loss: 0.0697\n",
      "  Validation Loss: 0.2041, Validation Accuracy: 93.63%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 40/75\n",
      "  Training Loss: 0.0746\n",
      "  Validation Loss: 0.2098, Validation Accuracy: 92.86%\n",
      "   No improvement (2/10)\n",
      "\n",
      "Epoch 41/75\n",
      "  Training Loss: 0.0633\n",
      "  Validation Loss: 0.2128, Validation Accuracy: 93.82%\n",
      "   No improvement (3/10)\n",
      "\n",
      "Epoch 42/75\n",
      "  Training Loss: 0.0649\n",
      "  Validation Loss: 0.2082, Validation Accuracy: 92.47%\n",
      "   No improvement (4/10)\n",
      "\n",
      "Epoch 43/75\n",
      "  Training Loss: 0.0547\n",
      "  Validation Loss: 0.1891, Validation Accuracy: 93.24%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 44/75\n",
      "  Training Loss: 0.0503\n",
      "  Validation Loss: 0.2102, Validation Accuracy: 93.24%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 45/75\n",
      "  Training Loss: 0.0506\n",
      "  Validation Loss: 0.2203, Validation Accuracy: 93.05%\n",
      "   No improvement (2/10)\n",
      "\n",
      "Epoch 46/75\n",
      "  Training Loss: 0.0496\n",
      "  Validation Loss: 0.2077, Validation Accuracy: 93.24%\n",
      "   No improvement (3/10)\n",
      "\n",
      "Epoch 47/75\n",
      "  Training Loss: 0.0458\n",
      "  Validation Loss: 0.1979, Validation Accuracy: 94.02%\n",
      "   No improvement (4/10)\n",
      "\n",
      "Epoch 48/75\n",
      "  Training Loss: 0.0405\n",
      "  Validation Loss: 0.1850, Validation Accuracy: 94.21%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 49/75\n",
      "  Training Loss: 0.0412\n",
      "  Validation Loss: 0.2262, Validation Accuracy: 93.05%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 50/75\n",
      "  Training Loss: 0.0405\n",
      "  Validation Loss: 0.1856, Validation Accuracy: 93.44%\n",
      "   No improvement (2/10)\n",
      "\n",
      "Epoch 51/75\n",
      "  Training Loss: 0.0406\n",
      "  Validation Loss: 0.1936, Validation Accuracy: 93.82%\n",
      "   No improvement (3/10)\n",
      "\n",
      "Epoch 52/75\n",
      "  Training Loss: 0.0361\n",
      "  Validation Loss: 0.1997, Validation Accuracy: 93.24%\n",
      "   No improvement (4/10)\n",
      "\n",
      "Epoch 53/75\n",
      "  Training Loss: 0.0322\n",
      "  Validation Loss: 0.1738, Validation Accuracy: 94.21%\n",
      "  Saved Model - New Best Validation Loss!\n",
      "\n",
      "Epoch 54/75\n",
      "  Training Loss: 0.0338\n",
      "  Validation Loss: 0.1799, Validation Accuracy: 94.21%\n",
      "   No improvement (1/10)\n",
      "\n",
      "Epoch 55/75\n",
      "  Training Loss: 0.0350\n",
      "  Validation Loss: 0.1874, Validation Accuracy: 93.82%\n",
      "   No improvement (2/10)\n",
      "\n",
      "Epoch 56/75\n",
      "  Training Loss: 0.0273\n",
      "  Validation Loss: 0.1864, Validation Accuracy: 94.21%\n",
      "   No improvement (3/10)\n",
      "\n",
      "Epoch 57/75\n",
      "  Training Loss: 0.0296\n",
      "  Validation Loss: 0.1890, Validation Accuracy: 94.02%\n",
      "   No improvement (4/10)\n",
      "\n",
      "Epoch 58/75\n",
      "  Training Loss: 0.0286\n",
      "  Validation Loss: 0.2226, Validation Accuracy: 92.66%\n",
      "   No improvement (5/10)\n",
      "\n",
      "Epoch 59/75\n",
      "  Training Loss: 0.0275\n",
      "  Validation Loss: 0.1874, Validation Accuracy: 94.21%\n",
      "   No improvement (6/10)\n",
      "\n",
      "Epoch 60/75\n",
      "  Training Loss: 0.0253\n",
      "  Validation Loss: 0.1911, Validation Accuracy: 93.63%\n",
      "   No improvement (7/10)\n",
      "\n",
      "Epoch 61/75\n",
      "  Training Loss: 0.0241\n",
      "  Validation Loss: 0.1923, Validation Accuracy: 94.40%\n",
      "   No improvement (8/10)\n",
      "\n",
      "Epoch 62/75\n",
      "  Training Loss: 0.0231\n",
      "  Validation Loss: 0.1974, Validation Accuracy: 93.24%\n",
      "   No improvement (9/10)\n",
      "\n",
      "Epoch 63/75\n",
      "  Training Loss: 0.0233\n",
      "  Validation Loss: 0.1929, Validation Accuracy: 93.82%\n",
      "   No improvement (10/10)\n",
      "\n",
      "⏹️Early stopping triggered after 63 epochs.\n",
      "\n",
      "FINISHED TRAINING!\n"
     ]
    }
   ],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimiser=torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimiser, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimiser, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3 \n",
    ")\n",
    "train(cnn, train_dataloader, validation_dataloader, loss_fn, optimiser, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7eec0baf-77b6-48b0-ac15-a7c995091791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping=[\n",
    "    \"dog_bark\",\n",
    "    \"drilling\",\n",
    "    \"engine_idling\",\n",
    "    \"siren\",\n",
    "    \"street_music\"\n",
    "]\n",
    "def predict(model, input, target, class_mapping):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        predicted_indices = torch.argmax(predictions, dim=1)\n",
    "        predicted_labels = [class_mapping[i.item()] for i in predicted_indices]\n",
    "\n",
    "        if target is not None:\n",
    "            expected = [class_mapping[t] for t in target]\n",
    "            return predicted_labels, expected\n",
    "        else:\n",
    "            return predicted_labels\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.files[index]\n",
    "        file_path = os.path.join(self.audio_dir, file_name)\n",
    "        signal, sr = torchaudio.load(file_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, file_name\n",
    "\n",
    "    \n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a56ceb04-9629-4748-becc-e495e4cc7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(\n",
    "    audio_dir=\"/Users/ramupadhyay/Desktop/test\",\n",
    "    transformation=mfcc_transformation,\n",
    "    target_sample_rate=SAMPLE_RATE,\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8e518a95-ec18-4df2-97f7-8a18dd06963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ram19_submission.csv created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cnn.load_state_dict(torch.load(\"fest_model_best.pth\"))\n",
    "cnn.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, file_names in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        predictions = cnn(inputs)\n",
    "        predicted_indices = predictions.argmax(dim=1)\n",
    "        for fname, pred_idx in zip(file_names, predicted_indices):\n",
    "            label = class_mapping[pred_idx.item()]\n",
    "            results.append((fname, label))\n",
    "\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame(results, columns=[\"ID\", \"Class\"])\n",
    "submission_df.to_csv(\"ram19_submission.csv\", index=False)\n",
    "print(\"ram19_submission.csv created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6df418-95cc-4868-9988-80214c3f1a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (audio)",
   "language": "python",
   "name": "audioenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
